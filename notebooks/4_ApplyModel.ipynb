{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Run Enriched Polygons Through Trained OSM Completeness Model</h1>\n",
    "<p>After enriching a collection of 250-m by 250-m grid cells, the output can be run through the trained OSM completeness model to produce predictions of OSM building footprint area.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import shape, box, mapping, Point, Polygon\n",
    "from matplotlib.patches import Polygon as mpoly\n",
    "import matplotlib\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.collections as collections\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>These are all variables that need to be set before running the notebook</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file containing trained model .sav file from training notebook\n",
    "trainedModel = 'path_to_trained_model.sav'\n",
    "\n",
    "# load model\n",
    "model = joblib.load(trainedModel)\n",
    "\n",
    "#predicted area value below which a cell will be considered \"no built up area\"\n",
    "noRoadThresh=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnFeatureVals(featureString, variable):\n",
    "    return [x['properties'][featureString] for x in variable['features']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file containing enriched cells on which to run the trained model\n",
    "filename = [f for f in glob.glob(\"path_to_enriched*.json\")]\n",
    "\n",
    "# what output json file should be called (cells containing predicted area and completeness values)\n",
    "outputFilename = []\n",
    "for i in enumerate(filename):\n",
    "    outputFilename.append(\"outputPath\"+str(i[0])+\".json\")\n",
    "    \n",
    "#directory for images to be output\n",
    "outputDirectory = 'path_to_outputDirectory'\n",
    "\n",
    "# lists for calculations\n",
    "mapped_list = []\n",
    "mapped_need_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for inputFile,file, outputFile in zip(enumerate(filename), filename, enumerate(outputFilename)):\n",
    "    \n",
    "    # 1. load data\n",
    "    with open(file,'r') as data:\n",
    "        x = json.load(data)\n",
    "        \n",
    "    # 2. create polygons for geodataframe\n",
    "    polygons=[]\n",
    "    for feature in x['features']:\n",
    "        g = shape(feature['geometry']).buffer(0)\n",
    "        polygons.append(g)\n",
    "    print(len(polygons))\n",
    "    \n",
    "    # function\n",
    "    def returnFeatureVals(featureString, variable):\n",
    "        return [x['properties'][featureString] for x in variable['features']]\n",
    "\n",
    "    # 3. set feature dictionary\n",
    "    applyFeatureDict = {\n",
    "        'ndbi':returnFeatureVals('ndbi', x),\n",
    "        'ndvi':returnFeatureVals('ndvi', x),\n",
    "        'savi':returnFeatureVals('savi', x),\n",
    "        'ui':returnFeatureVals('ui', x),\n",
    "        'viirs':returnFeatureVals('viirs', x),\n",
    "        'slope':returnFeatureVals('slope', x),\n",
    "        'texture':returnFeatureVals('texture', x),\n",
    "        'forest':returnFeatureVals('forest', x),\n",
    "        'ndbi_diff2018':returnFeatureVals('ndbi_diff2018', x),\n",
    "        'ndvi_diff2018':returnFeatureVals('ndvi_diff2018', x),\n",
    "        'savi_diff2018':returnFeatureVals('savi_diff2018', x),\n",
    "        'ui_diff2018':returnFeatureVals('ui_diff2018', x),\n",
    "        'ndbi_diff2017':returnFeatureVals('ndbi_diff2017', x),\n",
    "        'ndvi_diff2017':returnFeatureVals('ndvi_diff2017', x),\n",
    "        'savi_diff2017':returnFeatureVals('savi_diff2017', x),\n",
    "        'ui_diff2017':returnFeatureVals('ui_diff2017', x),\n",
    "        'ndbi_diff2016':returnFeatureVals('ndbi_diff2016', x),\n",
    "        'ndvi_diff2016':returnFeatureVals('ndvi_diff2016', x),\n",
    "        'savi_diff2016':returnFeatureVals('savi_diff2016', x),\n",
    "        'ui_diff2016':returnFeatureVals('ui_diff2016', x),\n",
    "        'ndbi_diff2015':returnFeatureVals('ndbi_diff2015', x),\n",
    "        'ndvi_diff2015':returnFeatureVals('ndvi_diff2015', x),\n",
    "        'savi_diff2015':returnFeatureVals('savi_diff2015', x),\n",
    "        'ui_diff2015':returnFeatureVals('ui_diff2015', x),\n",
    "        'MERIT':returnFeatureVals('MERIT', x),\n",
    "        'ndbiUSGS':returnFeatureVals('ndbiUSGS', x),\n",
    "        'ndviUSGS':returnFeatureVals('ndviUSGS', x),\n",
    "        'saviUSGS':returnFeatureVals('saviUSGS', x),\n",
    "        'PopDen':returnFeatureVals('PopDen', x),\n",
    "        'PopDenUN':returnFeatureVals('PopDenUN', x), \n",
    "        'length':returnFeatureVals('length',x)\n",
    "    }\n",
    "    \n",
    "    # 4. Apply dict\n",
    "    applyDF = pd.DataFrame.from_dict(applyFeatureDict)\n",
    "    applyGeoDF = gpd.GeoDataFrame(applyDF,crs = 4326, geometry=polygons)\n",
    "    applyGeoDF = applyGeoDF.fillna(0)\n",
    "    \n",
    "    # 5. apply features\n",
    "    applyFeatureDF = applyGeoDF[['ndbi',\n",
    "                                   'ndvi',\n",
    "                                   'savi',\n",
    "                                   'ui',\n",
    "                                   'ndbi_diff2018', \n",
    "                                    'ndvi_diff2018', \n",
    "                                    'savi_diff2018', \n",
    "                                    'ui_diff2018', \n",
    "                                    'ndbi_diff2017',\n",
    "                                    'ndvi_diff2017',\n",
    "                                    'savi_diff2017',\n",
    "                                    'ui_diff2017',\n",
    "                                    'ndbi_diff2016',\n",
    "                                    'ndvi_diff2016',\n",
    "                                    'savi_diff2016',\n",
    "                                    'ui_diff2016',\n",
    "                                    'ndbi_diff2015',\n",
    "                                    'ndvi_diff2015',\n",
    "                                    'savi_diff2015',\n",
    "                                    'ui_diff2015',\n",
    "                                   'viirs',\n",
    "                                   'slope',\n",
    "                                   'texture',\n",
    "                                   'forest',\n",
    "                                   'MERIT', \n",
    "                                   'ndbiUSGS', \n",
    "                                   'ndviUSGS', \n",
    "                                   'saviUSGS', \n",
    "                                   'PopDen', \n",
    "                                   'PopDenUN'\n",
    "                                   ]]\n",
    "    applyTargetDF = applyGeoDF['length']\n",
    "    \n",
    "    # 5. apply model\n",
    "    y_apply = model.predict(applyFeatureDF)\n",
    "\n",
    "    \n",
    "    # 6. generate data\n",
    "    for i,feature in enumerate(x['features']):\n",
    "        length = x['features'][i]['properties']['length']\n",
    "        #erase other properties for smaller output file\n",
    "        feature['properties'] = {}\n",
    "        #predicted OSM road footprint area\n",
    "        feature['properties']['plength'] = y_apply[i]\n",
    "        #actual mapped road footprint area\n",
    "        feature['properties']['builtLength'] = length\n",
    "        \n",
    "    # 7. save output\n",
    "    with open(outputFile[1],'w') as f:\n",
    "        json.dump(x,f)\n",
    "        \n",
    "    # 8. visualization\n",
    "    #import as dictionary\n",
    "    with open(outputFile[1],\"r\") as data:\n",
    "        x = json.load(data)\n",
    "    #load as dataframe\n",
    "    plotdf = gpd.read_file(outputFile[1])\n",
    "\n",
    "    bounds = plotdf.total_bounds\n",
    "\n",
    "    bounds = [bounds[0], bounds[2], bounds[1], bounds[3]]\n",
    "\n",
    "    clat = (bounds[2]+bounds[3])/2\n",
    "    clon = (bounds[0]+bounds[1])/2\n",
    "\n",
    "    features = x['features']\n",
    "    polys=[]\n",
    "    #predicted area\n",
    "    vals = []\n",
    "    #completeness\n",
    "    vals2 = []\n",
    "    lws=[]\n",
    "    for i,feature in enumerate(features):\n",
    "        if (i%1000)==0:\n",
    "            print(f'{i} of {len(features)}')\n",
    "\n",
    "        coords = feature['geometry']['coordinates'][0]\n",
    "        x=[]\n",
    "        y=[]\n",
    "\n",
    "        vals.append(feature['properties']['plength'])\n",
    "\n",
    "        try:\n",
    "            vals2.append(feature['properties']['builtLength']/feature['properties']['plength'])\n",
    "        except:\n",
    "            vals2.append(0)\n",
    "\n",
    "        for point in coords:\n",
    "            x.append(point[0])\n",
    "            y.append(point[1])\n",
    "        transformed = ccrs.LambertConformal(central_latitude=clat,central_longitude=clon).transform_points(ccrs.PlateCarree(),np.asarray(x),np.asarray(y))\n",
    "        polys.append(mpoly(transformed[:,0:2]))\n",
    "        lws.append(0.05)\n",
    "\n",
    "    cmap = matplotlib.cm.get_cmap('viridis')\n",
    "    cmap2 = matplotlib.cm.get_cmap('RdYlGn')\n",
    "    \n",
    "    #norm = matplotlib.colors.Normalize(vmin=0,vmax=np.nanmax(vals))\n",
    "    norm = matplotlib.colors.Normalize(vmin=0,vmax=4000)\n",
    "    norm2 = matplotlib.colors.Normalize(vmin=0,vmax=1)\n",
    "    fcs = cmap(norm(vals))\n",
    "    vals2 = np.asarray(vals2)\n",
    "    vals2[vals2>1] = 1\n",
    "    fcs2 = cmap2(norm2(vals2))\n",
    "\n",
    "    fcs2[np.asarray(vals)<noRoadThresh] = (.9,.9,.9,1)\n",
    "\n",
    "    #pc=PatchCollection(polys, facecolors=fcs,edgecolors=fcs,linewidths=lws)   \n",
    "    #pc2=PatchCollection(polys, facecolors=fcs2,edgecolors=fcs2,linewidths=lws)   \n",
    "\n",
    "    pc=collections.PatchCollection(polys, facecolors=fcs,edgecolors=fcs,linewidths=lws)   \n",
    "    pc2=collections.PatchCollection(polys, facecolors=fcs2,edgecolors=fcs2,linewidths=lws)   \n",
    "\n",
    "    # image 1\n",
    "    fig = plt.figure(constrained_layout = True)\n",
    "    gs = fig.add_gridspec(20,10)\n",
    "    ax1 = fig.add_subplot(gs[0:19,:],projection=ccrs.LambertConformal(central_latitude=clat,central_longitude=clon))\n",
    "    ax2= fig.add_subplot(gs[19::,:])\n",
    "    ax1.set_extent(bounds)\n",
    "    ax1.set_title('Predicted Length',fontsize=10)\n",
    "    ax1.add_collection(pc)\n",
    "    cb=fig.colorbar(matplotlib.cm.ScalarMappable(norm=norm, cmap=cmap),\n",
    "                 cax=ax2, orientation='horizontal')\n",
    "    cb.ax.tick_params(labelsize=8)\n",
    "    cb.set_label(label=\"Length (meters)\",fontsize=8)\n",
    "    plt.savefig(outputDirectory+'predicted'+str(inputFile[0])+'.png',bbox_inches='tight',dpi=100) # dpi=1000\n",
    "    plt.close(fig) \n",
    "\n",
    "\n",
    "    # image 2\n",
    "    fig = plt.figure(constrained_layout = True)\n",
    "    gs = fig.add_gridspec(20,10)\n",
    "    ax1 = fig.add_subplot(gs[0:19,:],projection=ccrs.LambertConformal(central_latitude=clat,central_longitude=clon))\n",
    "    ax2= fig.add_subplot(gs[19::,:])\n",
    "    ax1.set_extent(bounds)\n",
    "    ax1.set_title('Ratio Predicted to Actual Length',fontsize=10)\n",
    "    ax1.add_collection(pc2)\n",
    "    cb=fig.colorbar(matplotlib.cm.ScalarMappable(norm=norm2, cmap=cmap2),\n",
    "                 cax=ax2, orientation='horizontal')\n",
    "    cb.ax.tick_params(labelsize=8)\n",
    "    cb.set_label(label=\"Ratio\",fontsize=8)\n",
    "    plt.savefig(outputDirectory+'ratio'+str(inputFile[0])+'.png',bbox_inches='tight',dpi=100) # dpi=1000\n",
    "    plt.close(fig)\n",
    "    \n",
    "\n",
    "    # 9. calculations\n",
    "    #quantitative assessment of unmapped footprint area. Assumes a completeness ratio > 0.5 is mapped\n",
    "    builtUpIdx = np.nonzero(np.asarray(vals)>=noRoadThresh)\n",
    "    builtUp = vals2[builtUpIdx]\n",
    "\n",
    "    areasBuilt = np.asarray(vals)[builtUpIdx]\n",
    "\n",
    "    completeIdx = np.nonzero(builtUp>=0.5)\n",
    "    incompleteIdx = np.nonzero(builtUp<0.5)\n",
    "    completeAreas = areasBuilt[completeIdx]\n",
    "    incompleteAreas = areasBuilt[incompleteIdx]\n",
    "    complete = builtUp[completeIdx]\n",
    "    incomplete = builtUp[incompleteIdx]\n",
    "    mappedBuildings=np.sum(completeAreas)\n",
    "    unmappedBuildings=np.sum(incompleteAreas)\n",
    "    \n",
    "    mapped_list.append(mappedBuildings)\n",
    "    mapped_need_list.append(unmappedBuildings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe to show mapped / unmapped / %mapped for each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame((list(zip(mapped_list, mapped_need_list))))\n",
    "df.columns = ['mapped', 'unmapped']\n",
    "df['%mapped'] = 100*df['mapped']/(df['mapped']+df['unmapped'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaling = MinMaxScaler() # object\n",
    "#values between 0 and 1\n",
    "norm = scaling.fit_transform(df[['%mapped']]) # pass only certain columns \n",
    "df['%mapped_norm'] = norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df\n",
    "df.to_csv('../data/4_applymodel/grid_03/final_calculations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution\n",
    "hist = df.hist(figsize=(15,10), bins=10)\n",
    "hist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
